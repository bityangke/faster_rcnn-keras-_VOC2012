{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:14:28.878842Z",
     "start_time": "2019-10-18T13:14:28.871700Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config =  tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5 # 占用GPU30%的显存 \n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:14:29.361876Z",
     "start_time": "2019-10-18T13:14:29.196752Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "# from keras_frcnn import config, data_generators\n",
    "# from keras_frcnn import losses as losses\n",
    "# import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras.utils import generic_utils\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:14:29.946859Z",
     "start_time": "2019-10-18T13:14:29.939429Z"
    }
   },
   "outputs": [],
   "source": [
    "import resnet50 as nn\n",
    "import losses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:14:31.828280Z",
     "start_time": "2019-10-18T13:14:31.816336Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设置递归深度的限制\n",
    "sys.setrecursionlimit(40000)\n",
    "\n",
    "verbose = True\n",
    "network = 'resnet50'\n",
    "\n",
    "use_horizontal_flips = False\n",
    "use_vertical_flips = False\n",
    "rot_90 = False\n",
    "\n",
    "anchor_box_scales = [128, 256, 512]\n",
    "anchor_box_ratios = [[1, 1], [1, 2], [2, 1]]\n",
    "\n",
    "im_size = 600\n",
    "img_channel_mean = [103.939, 116.779, 123.68]\n",
    "img_scaling_factor = 1.0\n",
    "num_rois = 300\n",
    "rpn_stride = 16\n",
    "balanced_classes = False\n",
    "std_scaling = 4.0\n",
    "classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "rpn_min_overlap = 0.3\n",
    "rpn_max_overlap = 0.7\n",
    "\n",
    "classifier_min_overlap = 0.1\n",
    "classifier_max_overlap = 0.5\n",
    "class_mapping = None\n",
    "model_path = './model_frcnn_resnet.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:14:40.573625Z",
     "start_time": "2019-10-18T13:14:40.561326Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集VOC2012所在的文件夹\n",
    "train_path = \"/home/amax/Documents/\"\n",
    "parser =\"pascal_voc\"\n",
    "num_rois = 32\n",
    "network = 'resnet50'\n",
    "\n",
    "horizontal_flips = False\n",
    "vertical_flips = False\n",
    "rot_90 = False\n",
    "\n",
    "num_epochs = 500                \n",
    "config_filename = \"config.pickle\"\n",
    "# 训练后权重文件路径\n",
    "output_weight_path = './model_frcnn.hdf5'\n",
    "input_weight_path = \"resnet50_weights_tf_dim_ordering_tf_kernels.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:15:08.349624Z",
     "start_time": "2019-10-18T13:14:44.526530Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2007_002895.xml:   0%|          | 52/17125 [00:00<00:32, 519.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 2010_002187.xml: 100%|██████████| 17125/17125 [00:23<00:00, 722.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_data(input_path):\n",
    "    all_imgs = []\n",
    "    classes_count = {}\n",
    "    # 类别映射\n",
    "    class_mapping = {}\n",
    "\n",
    "    visualise = False\n",
    "\n",
    "    data_paths = [os.path.join(input_path, 'VOC2012')]\n",
    "\n",
    "    print('Parsing annotation files')\n",
    "    for data_path in data_paths:\n",
    "\n",
    "        annot_path = os.path.join(data_path, 'Annotations')\n",
    "        imgs_path = os.path.join(data_path, 'JPEGImages')\n",
    "\n",
    "        imgsets_path_trainval = os.path.join(data_path, 'ImageSets', 'Main', 'trainval.txt')\n",
    "        imgsets_path_train = os.path.join(data_path, 'ImageSets', 'Main', 'train.txt')\n",
    "        imgsets_path_val = os.path.join(data_path, 'ImageSets', 'Main', 'val.txt')\n",
    "        imgsets_path_test = os.path.join(data_path, 'ImageSets', 'Main', 'test.txt')\n",
    "\n",
    "        trainval_files = []\n",
    "        train_files = []\n",
    "        val_files = []\n",
    "        test_files = []\n",
    "\n",
    "        with open(imgsets_path_trainval) as f:\n",
    "            for line in f:\n",
    "                trainval_files.append(line.strip() + '.jpg')\n",
    "\n",
    "        with open(imgsets_path_train) as f:\n",
    "            for line in f:\n",
    "                train_files.append(line.strip() + '.jpg')\n",
    "\n",
    "        with open(imgsets_path_val) as f:\n",
    "            for line in f:\n",
    "                val_files.append(line.strip() + '.jpg')\n",
    "\n",
    "        if os.path.isfile(imgsets_path_test):\n",
    "            with open(imgsets_path_test) as f:\n",
    "                for line in f:\n",
    "                    test_files.append(line.strip() + '.jpg')\n",
    "\n",
    "        annots = [os.path.join(annot_path, s) for s in os.listdir(annot_path)]\n",
    "        idx = 0\n",
    "\n",
    "        annots = tqdm(annots)\n",
    "        for annot in annots:\n",
    "            # try:\n",
    "            exist_flag = False\n",
    "            idx += 1\n",
    "            annots.set_description(\"Processing %s\" % annot.split(os.sep)[-1])\n",
    "\n",
    "            et = ET.parse(annot)\n",
    "            element = et.getroot()\n",
    "\n",
    "            element_objs = element.findall('object')\n",
    "            element_filename = element.find('filename').text\n",
    "            element_width = int(element.find('size').find('width').text)\n",
    "            element_height = int(element.find('size').find('height').text)\n",
    "\n",
    "            if len(element_objs) > 0:\n",
    "                annotation_data = {'filepath': os.path.join(imgs_path, element_filename), 'width': element_width,\n",
    "                                   'height': element_height, 'bboxes': []}\n",
    "\n",
    "                annotation_data['image_id'] = idx\n",
    "\n",
    "                if element_filename in trainval_files:\n",
    "                    annotation_data['imageset'] = 'trainval'\n",
    "                    exist_flag = True\n",
    "\n",
    "                if element_filename in train_files:\n",
    "                    annotation_data['imageset'] = 'train'\n",
    "                    exist_flag = True\n",
    "\n",
    "                if element_filename in val_files:\n",
    "                    annotation_data['imageset'] = 'val'\n",
    "                    exist_flag = True\n",
    "\n",
    "                if len(test_files) > 0:\n",
    "                    if element_filename in test_files:\n",
    "                        annotation_data['imageset'] = 'test'\n",
    "                        exist_flag = True\n",
    "\n",
    "            # annotation file not exist in ImageSet\n",
    "            if not exist_flag:\n",
    "                continue\n",
    "\n",
    "            for element_obj in element_objs:\n",
    "                class_name = element_obj.find('name').text\n",
    "                if class_name not in classes_count:\n",
    "                    classes_count[class_name] = 1\n",
    "                else:\n",
    "                    classes_count[class_name] += 1\n",
    "\n",
    "                # class mapping \n",
    "                if class_name not in class_mapping:\n",
    "                    class_mapping[class_name] = len(class_mapping) \n",
    "\n",
    "                obj_bbox = element_obj.find('bndbox')\n",
    "                x1 = int(round(float(obj_bbox.find('xmin').text)))\n",
    "                y1 = int(round(float(obj_bbox.find('ymin').text)))\n",
    "                x2 = int(round(float(obj_bbox.find('xmax').text)))\n",
    "                y2 = int(round(float(obj_bbox.find('ymax').text)))\n",
    "                difficulty = int(element_obj.find('difficult').text) == 1\n",
    "                annotation_data['bboxes'].append(\n",
    "                    {'class': class_name, 'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'difficult': difficulty})\n",
    "            all_imgs.append(annotation_data)\n",
    "\n",
    "            if visualise:\n",
    "                img = cv2.imread(annotation_data['filepath'])\n",
    "                for bbox in annotation_data['bboxes']:\n",
    "                    cv2.rectangle(img, (bbox['x1'], bbox['y1']), (bbox['x2'], bbox['y2']), (0, 0, 255))\n",
    "                cv2.imshow('img', img)\n",
    "                print(annotation_data['imageset'])\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(e)\n",
    "            #     continue\n",
    "    return all_imgs, classes_count, class_mapping\n",
    "\n",
    "all_imgs, classes_count, class_mapping = get_data(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T05:43:28.542838Z",
     "start_time": "2019-10-19T05:43:28.534457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11540\n",
      "{'chair': 2906, 'bottle': 1482, 'boat': 999, 'pottedplant': 1099, 'cow': 702, 'motorbike': 751, 'horse': 750, 'cat': 1227, 'train': 656, 'diningtable': 747, 'bicycle': 790, 'bird': 1221, 'tvmonitor': 826, 'bg': 0, 'person': 10129, 'dog': 1541, 'sheep': 994, 'car': 2364, 'sofa': 786, 'aeroplane': 954, 'bus': 637}\n"
     ]
    }
   ],
   "source": [
    "print(len(all_imgs))\n",
    "print(classes_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:16:11.377316Z",
     "start_time": "2019-10-18T13:16:11.310756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images per class:\n",
      "{'aeroplane': 954,\n",
      " 'bg': 0,\n",
      " 'bicycle': 790,\n",
      " 'bird': 1221,\n",
      " 'boat': 999,\n",
      " 'bottle': 1482,\n",
      " 'bus': 637,\n",
      " 'car': 2364,\n",
      " 'cat': 1227,\n",
      " 'chair': 2906,\n",
      " 'cow': 702,\n",
      " 'diningtable': 747,\n",
      " 'dog': 1541,\n",
      " 'horse': 750,\n",
      " 'motorbike': 751,\n",
      " 'person': 10129,\n",
      " 'pottedplant': 1099,\n",
      " 'sheep': 994,\n",
      " 'sofa': 786,\n",
      " 'train': 656,\n",
      " 'tvmonitor': 826}\n",
      "Num classes (including bg) = 21\n",
      "Num train samples 5717\n",
      "Num val samples 5823\n",
      "Num test samples 0\n"
     ]
    }
   ],
   "source": [
    "# 如果没有背景类，向里面加入背景类\n",
    "if 'bg' not in classes_count:\n",
    "    classes_count['bg'] = 0\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "# 将class_mapping以字典形式保存\n",
    "inv_map = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
    "\n",
    "# 打乱数据\n",
    "random.shuffle(all_imgs)\n",
    "\n",
    "num_imgs = len(all_imgs)\n",
    "\n",
    "train_imgs = [s for s in all_imgs if s['imageset'] == 'train']\n",
    "val_imgs = [s for s in all_imgs if s['imageset'] == 'val']\n",
    "test_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
    "\n",
    "#显示训练集，验证集，测试集的数量\n",
    "print('Num train samples {}'.format(len(train_imgs)))\n",
    "print('Num val samples {}'.format(len(val_imgs)))\n",
    "print('Num test samples {}'.format(len(test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:16:12.305015Z",
     "start_time": "2019-10-18T13:16:12.293622Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据增强\n",
    "import data_generator\n",
    "import resnet60_Copy1 as nn\n",
    "data_gen_train = data_generator.get_anchor_gt(train_imgs, classes_count,nn.get_img_output_length,mode='train')\n",
    "data_gen_val = data_generator.get_anchor_gt(val_imgs, classes_count, nn.get_img_output_length,mode='val')\n",
    "data_gen_test = data_generator.get_anchor_gt(test_imgs, classes_count,nn.get_img_output_length, mode='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:16:15.668347Z",
     "start_time": "2019-10-18T13:16:13.388803Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape_img = (None, None, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "anchor_box_scales = [128, 256, 512]\n",
    "anchor_box_ratios = [[1, 1], [1, 2], [2, 1]]\n",
    "num_anchors = len(anchor_box_scales) * len(anchor_box_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:16:24.803409Z",
     "start_time": "2019-10-18T13:16:15.817564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from resnet50_weights_tf_dim_ordering_tf_kernels.h5\n"
     ]
    }
   ],
   "source": [
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(shared_layers, roi_input, num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "# 此处最好先下载好模型，初始化模型的位置input_weight_path\n",
    "try:\n",
    "    print('loading weights from {}'.format(input_weight_path))\n",
    "    model_rpn.load_weights(input_weight_path, by_name=True)\n",
    "    model_classifier.load_weights(input_weight_path, by_name=True)\n",
    "except:\n",
    "    print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n",
    "        https://github.com/fchollet/keras/tree/master/keras/applications')\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "log_path = './logs'\n",
    "if not os.path.isdir(log_path):\n",
    "    os.mkdir(log_path)\n",
    "    \n",
    "callback = TensorBoard(log_path)\n",
    "callback.set_model(model_all)\n",
    "\n",
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:16:24.815805Z",
     "start_time": "2019-10-18T13:16:24.808420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chair': 4, 'bottle': 7, 'boat': 15, 'pottedplant': 3, 'cow': 0, 'motorbike': 13, 'horse': 14, 'cat': 9, 'train': 19, 'diningtable': 6, 'bicycle': 2, 'bird': 12, 'tvmonitor': 10, 'bg': 20, 'person': 1, 'dog': 16, 'sheep': 17, 'car': 11, 'sofa': 8, 'aeroplane': 5, 'bus': 18}\n"
     ]
    }
   ],
   "source": [
    "class_mapping = inv_map\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:16:24.828257Z",
     "start_time": "2019-10-18T13:16:24.819608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    }
   ],
   "source": [
    "epoch_length = 1000\n",
    "num_epochs = int(num_epochs)\n",
    "iter_num = 0\n",
    "train_step = 0\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in class_mapping.items()}\n",
    "print('Starting training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-18T13:16:24.836798Z",
     "start_time": "2019-10-18T13:16:24.831123Z"
    }
   },
   "outputs": [],
   "source": [
    "import roi_helpers as roi_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-19T06:31:11.339847Z",
     "start_time": "2019-10-19T05:46:20.418046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " 556/1000 [===============>..............] - ETA: 35:47 - rpn_cls: 3.9993 - rpn_regr: 0.2086 - detector_cls: 1.5883 - detector_regr: 0.4477"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7b405cda2204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mwrite_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'rpn_cls_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rpn_reg_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_rpn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mP_rpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroi_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn_to_roi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP_rpn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP_rpn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dim_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_regr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_boxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1943\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1944\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1945\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1946\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1947\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/myproject-env/tf1_9/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_num in range(num_epochs):\n",
    "\n",
    "    progbar = generic_utils.Progbar(epoch_length)  \n",
    "    print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "\n",
    "    while True:\n",
    "        # try:\n",
    "        # mean overlapping bboxes \n",
    "        if len(rpn_accuracy_rpn_monitor) == epoch_length and verbose:\n",
    "            mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "            rpn_accuracy_rpn_monitor = []\n",
    "            print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
    "            if mean_overlapping_bboxes == 0:\n",
    "                print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "        X, Y, img_data = next(data_gen_train)\n",
    "        \n",
    "\n",
    "        loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "        write_log(callback, ['rpn_cls_loss', 'rpn_reg_loss'], loss_rpn, train_step)\n",
    "\n",
    "        P_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "        R = roi_helper.rpn_to_roi(P_rpn[0], P_rpn[1], K.image_dim_ordering(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "        X2, Y1, Y2, IouS = roi_helper.calc_iou(R, img_data, class_mapping)\n",
    "\n",
    "        if X2 is None:\n",
    "            rpn_accuracy_rpn_monitor.append(0)\n",
    "            rpn_accuracy_for_epoch.append(0)\n",
    "            continue\n",
    "\n",
    "        # sampling positive/negative samples\n",
    "        neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "        pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "        if len(neg_samples) > 0:\n",
    "            neg_samples = neg_samples[0]\n",
    "        else:\n",
    "            neg_samples = []\n",
    "\n",
    "        if len(pos_samples) > 0:\n",
    "            pos_samples = pos_samples[0]\n",
    "        else:\n",
    "            pos_samples = []\n",
    "\n",
    "        rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "        rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "        if num_rois > 1:\n",
    "            if len(pos_samples) < num_rois//2:\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "            else:\n",
    "                selected_pos_samples = np.random.choice(pos_samples, num_rois//2, replace=False).tolist()\n",
    "            try:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "            except:\n",
    "                selected_neg_samples = np.random.choice(neg_samples, num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "            sel_samples = selected_pos_samples + selected_neg_samples\n",
    "        else:\n",
    "            # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "            selected_pos_samples = pos_samples.tolist()\n",
    "            selected_neg_samples = neg_samples.tolist()\n",
    "            if np.random.randint(0, 2):\n",
    "                sel_samples = random.choice(neg_samples)\n",
    "            else:\n",
    "                sel_samples = random.choice(pos_samples)\n",
    "        loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "        write_log(callback, ['detection_cls_loss', 'detection_reg_loss', 'detection_acc'], loss_class, train_step)\n",
    "        train_step += 1\n",
    "\n",
    "        losses[iter_num, 0] = loss_rpn[1]\n",
    "        losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "        losses[iter_num, 2] = loss_class[1]\n",
    "        losses[iter_num, 3] = loss_class[2]\n",
    "        losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "        progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                  ('detector_cls', np.mean(losses[:iter_num, 2])), ('detector_regr', np.mean(losses[:iter_num, 3]))])\n",
    "\n",
    "        if iter_num == epoch_length:\n",
    "            loss_rpn_cls = np.mean(losses[:, 0])\n",
    "            loss_rpn_regr = np.mean(losses[:, 1])\n",
    "            loss_class_cls = np.mean(losses[:, 2])\n",
    "            loss_class_regr = np.mean(losses[:, 3])\n",
    "            class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "            mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "            rpn_accuracy_for_epoch = []\n",
    "\n",
    "            if verbose:\n",
    "                print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "                print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "                print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "                print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "                print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "                print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "                print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "            curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "            iter_num = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            write_log(callback,\n",
    "                      ['Elapsed_time', 'mean_overlapping_bboxes', 'mean_rpn_cls_loss', 'mean_rpn_reg_loss',\n",
    "                       'mean_detection_cls_loss', 'mean_detection_reg_loss', 'mean_detection_acc', 'total_loss'],\n",
    "                      [time.time() - start_time, mean_overlapping_bboxes, loss_rpn_cls, loss_rpn_regr,\n",
    "                       loss_class_cls, loss_class_regr, class_acc, curr_loss],\n",
    "                      epoch_num)\n",
    "\n",
    "            if curr_loss < best_loss:\n",
    "                if verbose:\n",
    "                    print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                best_loss = curr_loss\n",
    "                model_all.save_weights(model_path)\n",
    "\n",
    "            break\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print('Exception: {}'.format(e))\n",
    "        #     continue\n",
    "\n",
    "print('Training complete, exiting.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-17T13:40:07.782Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "import time\n",
    "import resnet60_Copy1 as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-17T13:40:07.783Z"
    }
   },
   "outputs": [],
   "source": [
    "num_rois = 32\n",
    "img_path = test_path = \"./images\"\n",
    "network = \"resnet50\"\n",
    "use_horizontal_flips = False\n",
    "use_vertical_flips = False\n",
    "rot_90 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-17T13:40:07.785Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_img_size(img):\n",
    "    \"\"\" formats the image size based on config \"\"\"\n",
    "    img_min_side = float(im_size)\n",
    "    (height, width ,_) = img.shape\n",
    "\n",
    "    if width <= height:\n",
    "        ratio = img_min_side/width\n",
    "        new_height = int(ratio * height)\n",
    "        new_width = int(img_min_side)\n",
    "    else:\n",
    "        ratio = img_min_side/height\n",
    "        new_width = int(ratio * width)\n",
    "        new_height = int(img_min_side)\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    return img, ratio\n",
    "\n",
    "def format_img_channels(img):\n",
    "    \"\"\" formats the image channels based on config \"\"\"\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = img.astype(np.float32)\n",
    "    img[:, :, 0] -= img_channel_mean[0]\n",
    "    img[:, :, 1] -= img_channel_mean[1]\n",
    "    img[:, :, 2] -= img_channel_mean[2]\n",
    "    img /= img_scaling_factor\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def format_img(img):\n",
    "    \"\"\" formats an image for model prediction based on config \"\"\"\n",
    "    img, ratio = format_img_size(img)\n",
    "    img = format_img_channels(img)\n",
    "    return img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "    real_x1 = int(round(x1 // ratio))\n",
    "    real_y1 = int(round(y1 // ratio))\n",
    "    real_x2 = int(round(x2 // ratio))\n",
    "    real_y2 = int(round(y2 // ratio))\n",
    "\n",
    "    return (real_x1, real_y1, real_x2 ,real_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-17T13:40:07.787Z"
    }
   },
   "outputs": [],
   "source": [
    "if 'bg' not in class_mapping:\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)\n",
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\n",
    "num_rois = int(num_rois)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-17T13:40:07.789Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features = 1024\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape_img = (3, None, None)\n",
    "    input_shape_features = (num_features, None, None)\n",
    "else:\n",
    "    input_shape_img = (None, None, 3)\n",
    "    input_shape_features = (None, None, num_features)\n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (resnet here, can be VGG, Inception, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(anchor_box_scales) * len(anchor_box_ratios)\n",
    "rpn_layers = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(feature_map_input, roi_input, num_rois, nb_classes=len(class_mapping), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "print('Loading weights from {}'.format(model_path))\n",
    "model_rpn.load_weights(model_path, by_name=True)\n",
    "model_classifier.load_weights(model_path, by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-17T13:40:07.791Z"
    }
   },
   "outputs": [],
   "source": [
    "all_imgs = []\n",
    "\n",
    "classes = {}\n",
    "\n",
    "bbox_threshold = 0.8\n",
    "\n",
    "visualise = True\n",
    "\n",
    "for idx, img_name in enumerate(sorted(os.listdir(img_path))):\n",
    "    if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "        continue\n",
    "    print(img_name)\n",
    "    st = time.time()\n",
    "    filepath = os.path.join(img_path,img_name)\n",
    "\n",
    "    img = cv2.imread(filepath)\n",
    "\n",
    "    X, ratio = format_img(img)\n",
    "\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "    # get the feature maps and output from the RPN\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "\n",
    "    R = roi_helpers.rpn_to_roi(Y1, Y2,  K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "\n",
    "    for jk in range(R.shape[0]//num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[num_rois*jk:num_rois*(jk+1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "\n",
    "        if jk == R.shape[0]//num_rois:\n",
    "            #pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0],num_rois,curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "\n",
    "        [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                tx /= classifier_regr_std[0]\n",
    "                ty /= classifier_regr_std[1]\n",
    "                tw /= classifier_regr_std[2]\n",
    "                th /= classifier_regr_std[3]\n",
    "                x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([rpn_stride*x, rpn_stride*y, rpn_stride*(x+w), rpn_stride*(y+h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "    all_dets = []\n",
    "\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "\n",
    "        new_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "            (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "\n",
    "            cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n",
    "\n",
    "            textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "            all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "            (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "            textOrg = (real_x1, real_y1-0)\n",
    "\n",
    "            cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n",
    "            cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "    print('Elapsed time = {}'.format(time.time() - st))\n",
    "    print(all_dets)\n",
    "    #cv2.imshow('img', img)\n",
    "    #cv2.waitKey(0)\n",
    "    cv2.imwrite('./results_imgs/{}.png'.format(idx),img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
